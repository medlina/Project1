---
title       : Time Series Forecasting
author      : Olena Medvedyeva
framework   : bootstrap    
highlighter : prettify  
hitheme     : twitter-bootstrap      
widgets     : [mathjax]           
ext_widgets : {rCharts: [libraries/nvd3]}
mode        : standalone 

---

```{r setup, tidy = F, cache = FALSE, echo = FALSE, include=FALSE}

library(bitops)
library(RCurl)
library(foreign)
library(tseries)
library(zoo)
library(xts)
library(TTR)
library(timeDate)
library(forecast)
library(ggplot2)
library(base64enc)
library(devtools)
library(rCharts)
library(knitr)
library(ggplot2)
options(
  rcharts.mode = 'iframesrc', 
  rcharts.cdn = TRUE,
  RCHART_WIDTH = 700,
  RCHART_HEIGHT = 400
)
knitr::opts_chunk$set(tidy = F, cache=FALSE, results = 'asis', comment = NA, message = T)

```

#### Data Example

``` {r, chart1, echo = F, warning=FALSE}

# Data preprocessing

url <- "https://raw.githubusercontent.com/medlina/Project1/master/data1.txt"
myData <- getURL(url, ssl.verifypeer = FALSE)
example0_ordered <- read.table(textConnection(myData), header=TRUE, quote="\"") 
#example0 <- read.table("~/Olena/hub82.txt", header=TRUE, quote="\"")
#example0_ordered <- example0[order(as.Date(example0$dt, format="%Y-%m-%d")),]
#example0_ordered$day <- as.matrix(c(1:nrow(example0)))
#example0_ordered$usage[181] <- 2.613879e+14
#example0_ordered$usage <- example0_ordered$usage/(1e+14)
example0_ordered$date <- as.Date(example0_ordered$date, format = "%Y-%m-%d")


passPlot = nPlot(usage ~ date, data = example0_ordered, type = 'lineWithFocusChart')
passPlot$yAxis(axisLabel = "Usage", width = 40 )
passPlot$xAxis(tickFormat="#!function(d) {return d3.time.format.utc('%Y-%m-%d')(new Date(d * 24 * 60 * 60 * 1000));}!#" )
passPlot$print("chart1", include_assets=TRUE, cdn=TRUE)
```

We can see from the plot that this time series could probably be described using an additive model, since the random fluctuations in the data are roughly constant in size over time.

#### Estimation of Missing Data with Mean Value

``` {r, echo = F, warning=FALSE}

if (!is.na(example0_ordered$usage)) {
  print("There are no Missing Data in this Data Set")
}

example0_ordered$usage[example0_ordered$usage == 0] <- NA
result <- shapiro.test(example0_ordered$usage)

if (result$p.value < 0.05 && is.na(example0_ordered$usage)) {
  print("Data are Normaly Distributed")
  print(result)
  
  # Distribution Plot
  
  x <- example0_ordered$usage 
  h <- hist(x, breaks=30, col="red", xlab = "usage", 
    main = "Histogram with Normal Curve") 
  xfit<-seq(min(x),max(x),length=40) 
  yfit<-dnorm(xfit,mean=mean(x),sd=sd(x)) 
  yfit <- yfit*diff(h$mids[1:2])*length(x) 
  lines(xfit, yfit, col="blue", lwd=2)
  
  # Missing Values Estimation
  
  example0_ordered$usage[example0_ordered$usage == NA] <- mean(example0_ordered$usage)
  
  # Data Plot with Estimated Missing Values
  
  passPlot = nPlot(usage ~ dt, data = example0_ordered, type = 'lineWithFocusChart')
  passPlot$xAxis(tickFormat="#!function(d) {return d3.time.format.utc('%Y-%m-%d')(new Date(d * 24 * 60 * 60 * 1000));}!#" )
  passPlot$print("chart2", include_assets=TRUE, cdn=TRUE)
  } else {if (result$p.value >= 0.05 && is.na(example0_ordered$usage)){
          print("Data are not Normaly Distributed")
          print(result)
          }
  }

```

#### Estimation of Missing Data with Linear Interpolation

``` {r, echo = F, warning=FALSE}

if (!is.na(example0_ordered$usage)) {
  print("There are no Missing Data in this Data Set")
}

if (result$p.value >= 0.05 && is.na(example0_ordered$usage)){
  
  # Linear Interpolation of Missing Data
  
  for (i in 2:nrow(example0)) {
    if (is.na(example0_ordered$usage[i])){
      for (k in i:nrow(example0_ordered)){
        if (!is.na(example0_ordered$usage[k])){
        index1 <- k
        }
      }
      example0_ordered$usage[i] <- example0_ordered$usage[i-1] + (example0_ordered$usage[index1] - example0_ordered$usage[i-1]) * (example0_ordered$day[i] - example0_ordered$day[i -1]) / (example0_ordered$day[index1] - example0_ordered$day[i - 1])
   }
  } 

  # Data Plot with Estimated Missing Values
  
  passPlot = nPlot(usage ~ dt, data = example0_ordered, type = 'lineWithFocusChart')
  passPlot$xAxis(tickFormat="#!function(d) {return d3.time.format.utc('%Y-%m-%d')(new Date(d * 24 * 60 * 60 * 1000));}!#" )
  passPlot$print("chart2", include_assets=TRUE, cdn=TRUE)
 } else { if (result$p.value >= 0.05 && is.na(example0_ordered$mbps_out_sum_ma)){
   print("Missing Data are already interpolated")}
 }

```

#### Decomposing Time Series

``` {r, echo = F, warning=FALSE}

# example_decompose <- decompose(example_ts) - Error: time series has no or less than 2 periods

```

Decomposing a time series is needed to separate it into constituent components. From the plot above, we can assume that our data consists of a trend component and an irregular component (without seasonal component). However, it is better to test for seasonality on annual data. Thus, let's try  to start with fitting a Linear Model to our Data.

#### AutoFitting Linear Model

``` {r, echo = F, warning=FALSE, message=FALSE}

model1 <- lm(example0_ordered$usage ~ example0_ordered$day)

# Plotting Data with fitted Linear Model

# predicts + interval
new_day <- seq(min(example0_ordered$day), max(example0_ordered$day), length.out = 182)
pred_lm <- predict(model1, newdata = data.frame(day = new_day), interval = 'confidence')

# plot
plot(example0_ordered$usage ~ example0_ordered$day, ylab='usage',xlab='date',type='o', col = "blue")
# add fill
# set alpha = 80 so it's relatively transparent
color <- rgb(190, 190, 190, alpha = 80, maxColorValue=255)
polygon(c(rev(new_day), new_day), c(rev(pred_lm[ ,3]), pred_lm[ ,2]), col = color, border = NA)
# model
abline(model1, col = "red")

# intervals
lines(new_day, pred_lm[ ,3], lty = 'dashed', col = 'red')
lines(new_day, pred_lm[ ,2], lty = 'dashed', col = 'red')

```

``` {r, echo = F, warning=FALSE, message=FALSE}

print("Confidence intervals are")
confint(model1)

```

From this output, we have determined that the intercept is 2.59 and the coefficient is 0.0001571. Therefore, the complete regression equation is Usage = 2.59 + 0.0001571 * Time. This equation tells us that the predicted usage almost will not increase with time. Suppose that our research question asks what the expected usage is. As follows, we can use the regression equation to calculate the answer to this question.

Let's check model summary

``` {r, echo = F, warning=FALSE, message=FALSE}

summary(model1)

```

0% R-squared indicates that the model explains none of the variability of the response data around its mean.

After fitting a regression model, we should consider various diagnostic
plots. In the case of time series regression, an important diagnostic plot is the
correlogram of the residuals:

``` {r, echo = F, warning=FALSE, message=FALSE}

par(mfrow=c(1,2))
plot(residuals(model1))
acf(residuals(model1), lag.max = 20, main = "ACF of residuals")

```

We can see that there is an autocorrelation left in the residuals (seen in the significant spike in the ACF plot). This suggests that the model can be improved. We will come back to improving later. At first, we will fit linear models that are calculated manualy. 

#### Manual Fitting Linear Model in Matrix Form

``` {r, echo = F, warning=FALSE}

X_1 <- as.matrix(example0_ordered$day)
ones <- as.matrix(rep(1,nrow(example0_ordered)))
x <- cbind(ones, X_1)
y <- as.matrix(example0_ordered$usage)
m <- nrow(example0_ordered)

# Matrix form

Beta <- solve(t(x) %*% x) %*% t(x) %*% y
print("Estimated Coeficients are")
print(Beta)

# Plotting Data with fitted Linear Model

plot(example0_ordered$usage ~ example0_ordered$day, ylab='usage',xlab='date',type='o', col = "blue")
abline(Beta[1], Beta[2], col = "green")

```

#### Linear Regression with Gradient Descent

``` {r, echo = F, warning=FALSE}

iterations <- 500000
theta <- as.matrix(c(0,0))
theta1 <- as.matrix(c(0,0))
alpha <- 0.0001

for (i in 1:iterations){
    
  theta1[1] <- theta[1] + alpha * (1/m) * sum((y - theta[1] - theta[2] * X_1)) 
  theta1[2] <- theta[2] + alpha * (1/m) * sum((y -  theta[1] - theta[2] * X_1) * X_1)
  theta <- theta1
}

print("Estimated Coeficients are")
print(theta)

# Plotting Data with fitted Linear Model

plot(example0_ordered$usage ~ example0_ordered$day, ylab='usage',xlab='date',type='o', col = "blue")
abline(theta[1], theta[2], col = "red")

```

We can see that both manually built models are almost the same as fitted using R command. 

#### Decomposing Non-Seasonal Data

Now, let's try to fit the data better with other models.

``` {r, echo = F, warning=FALSE}
#To estimate the trend component of a non-seasonal time series that can be described using an additive model, it is common to use a smoothing method, such as calculating the simple moving average of teh tiem series.



example_ts <- ts(example0_ordered$usage, frequency = 365, start=c(2014,1,1))
#example_SMA <- SMA(example_ts, n = 20)
#plot.ts(example_SMA)

#The data smoothed with a simple moving average of order 20 gives a clearer picture of the trend component, and we can see that the usage seems to increase for first fifty days, then deacrease for the next fifty days, and then increased again after that.Even thought the plot show us seasonal-like behaviour, decomposing this det of data is not possible. Probably, we need more data to judge.

```

#### Forecasts using Exponential Smoothing

Exponential smoothing can be used to make short-term forecasts for time series data. You can see from the plot that there is roughly constant level (the mean stays constant at about 2.6). Thus, we can make forecasts using simple exponential smoothing.

``` {r, echo = F, warning=FALSE}

example_forecasts <- HoltWinters(example_ts, beta=FALSE, gamma=FALSE)
plot(example_forecasts)
print(" ")
example_forecasts

```

The plot shows the original time series in black, and the forecasts as a red line. The time series of forecasts is much smoother than the time series of the original data here.

The output of HoltWinters() tells us that the estimated value of the alpha parameter is about 0.17. It means that older data points are weighted more heavily. 

``` {r, echo = F, warning=FALSE}

#example_forecasts$SSE
example_forecasts2 <- forecast.HoltWinters(example_forecasts, h = 20)
plot.forecast(example_forecasts2)

```

On this plot you can see prediction for upcoming 20 days with 80% and 95% prediction intervals.


If the predictive model cannot be improved upon, there should be no correlations between forecast errors for successive predictions. In other words, if there are correlations between forecast errors for successive predictions, it is likely that the simple exponential smoothing forecasts could be improved upon by another forecasting technique.

To figure out whether this is the case, we can obtain a correlogram of the in-sample forecast errors for lags 1-20. 

``` {r, echo = F, warning=FALSE}

acf(example_forecasts2$residuals, lag.max=20)

```

You can see from the sample correlogram that the autocorrelation at lag 1, 11 and 17 are just touching the significance bounds. To test whether there is significant evidence for non-zero correlations at lags 1-20, we can carry out a Ljung-Box test.

``` {r, echo = F, warning=FALSE}

test <- Box.test(example_forecasts2$residuals, lag  = 20, type="Ljung-Box")
print(test)

```

Here the Ljung-Box test statistic is 24.89, and the p-value is 0.2, so there is  evidence of non-zero autocorrelations in the in-sample forecast errors at lags 1-20.

Thus, this one predictive model can also be improved upon. 

``` {r, echo = F, warning=FALSE}

plotForecastErrors <- function(forecasterrors)
  {
     # make a histogram of the forecast errors:
     mybinsize <- IQR(forecasterrors)/4
     mysd   <- sd(forecasterrors)
     mymin  <- min(forecasterrors) - mysd*5
     mymax  <- max(forecasterrors) + mysd*3
     # generate normally distributed data with mean 0 and standard deviation mysd
     mynorm <- rnorm(10000, mean=0, sd=mysd)
     mymin2 <- min(mynorm)
     mymax2 <- max(mynorm)
     if (mymin2 < mymin) { mymin <- mymin2 }
     if (mymax2 > mymax) { mymax <- mymax2 }
     # make a red histogram of the forecast errors, with the normally distributed data overlaid:
     mybins <- seq(mymin, mymax, mybinsize)
     hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
     # freq=FALSE ensures the area under the histogram = 1
     # generate normally distributed data with mean 0 and standard deviation mysd
     myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
     # plot the normal curve as a blue line on top of the histogram of forecast errors:
     points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
  }

#plotForecastErrors(example_forecasts2$residuals)
#plotForecastErrors(residuals(model1))

```

#### ARIMA

While exponential smoothing methods do not make any assumptions about correlations between successive values of the time series, ARIMA models include an explicit statistical model for the irregular component of a time series, that allows non-zero autocorrelations in the irregular component.

#### Differencing a Time Series

If we start off with a non-stationary time series, first we need to difference the time series until we obtain a stationary time series. In our case the time series is stationary.

``` {r, echo = F, warning=FALSE}

#examplediff1 <- diff(example_ts, differences=1)
#plot.ts(examplediff1)

```

When our time series is stationary, the next step is to select the appropriate ARIMA model, which means finding the values of most appropriate values of p and q for an ARIMA(p,d,q) model. To do this, we need to examine the correlogram and partial correlogram of the stationary time series.

``` {r, echo = F, warning=FALSE}

par(mfrow = c(1, 2))
acf(example_ts, lag.max=20)  
pacf(example_ts, lag.max=20)

#acf(example_ts, lag.max=20, plot=FALSE)
#pacf(example_ts, lag.max=20, plot=FALSE) 

```

Since the correlogram tends to zero after lag 2, and the partial correlogram shows correlations at lag 1 and 11, this the following ARMA (autoregressive moving average) models are possible for the time series:

- an ARMA(2,0) model, that is, an autoregressive model of order p = 2

- an ARMA(0,2) model, that is, a moving average model of order q = 2, since the autocorrelogram is zero after lag 2

- an ARMA(p,q) model, that is, a mixed model with p and q greater than 0, since the autocorrelogram and partial correlogram tail off to zero (although the correlogram probably tails off to zero too abruptly for this model to be appropriate)

We use the principle of parsimony to decide which model is best: that is, we assume that the model with the fewest parameters is best. Thus, let's take the ARMA(2,0) model.

An ARIMA(2,0,0) model is an autoregressive model of order 2, or AR(2) model. 

#### Autofitting ARIMA models

``` {r, echo = F, warning=FALSE}

auto.arima(example_ts)
examplearima <- arima(example_ts, order=c(2,0,0)) 
#examplearima

```

As a result we are proposed to use ARIMA(3,0,0) model.But let's fix to our chosen model ARIMA(2,0,0).

#### ARIMA forecast

``` {r, echo = F, warning=FALSE}

examplearima_forecast <- forecast.Arima(examplearima, h = 20)
plot.forecast(examplearima_forecast)

```

We obtained forecast for the next 20 days usage with 80% and 95% prediction intervals.

Now it's a good idea to investigate whether the forrecast errors of an ARIMA model are normally distributed with mean zero and constant variance, and whether there are correlations between successive forecast errors.

#### ARIMA MODEL VALIDATION

``` {r, echo = F, warning=FALSE}

par(mfrow = c(1, 2))
acf(examplearima_forecast$residuals, lag.max=20)
plot.ts(examplearima_forecast$residuals)  

Box.test(examplearima_forecast$residuals, lag=20, type="Ljung-Box")

```

Since the correlogram shows only one exceding the significance bounds (which could happen due to chance), and p-value for the Ljung-Box test is 0.67, we can conclude that there is just small evidence for non-zero autocorrelations in the forecast errors at lags 1-20.

``` {r, echo = F, warning=FALSE}

plotForecastErrors(examplearima_forecast$residuals)

```

Since succesive forecast errors do not seem to be correlated, and the forecast errors seem to be normaly distributed with mean zero and constant variance, the ARIMA(2,0,0) (or AR(2)) does seem to provide an adequate predictive model.

For the last step let's compare results of our Linear and ARIMA models. For that we devided our Data Set into two parts - a training one (75% of all data points), and a test one (25% of all data points). After that we compare predictions that are built and real values from the testing set.

``` {r, echo = F, warning=FALSE}

sub <- floor(nrow(example0_ordered) * 0.75)
training <- training <- example0_ordered[1:sub,]
training_ts <- ts(training$usage, frequency = 365, start = c(2014,1,1))
testing <- example0_ordered[(sub+1):nrow(example0_ordered), ]

model2 <- lm(training$usage ~ training$day)
model2_forecast <- predict(model2, interval = 'confidence')
model3 <- arima(training_ts, order=c(2,0,0)) 
model3_forecast <- forecast.Arima(model3, h = 46)

plot(training$usage ~ training$day, xlim = c(0,nrow(example0_ordered)), col = "blue", type = 'o')
lines(testing$usage ~ testing$day, col = "green", type = 'o')
abline(model2, col = "red")
# intervals
lines(model2_forecast[ ,3], lty = 'dashed', col = 'red')
lines(model2_forecast[ ,2], lty = 'dashed', col = 'red')


plot(training$usage ~ training$day, xlim = c(0,nrow(example0_ordered)), col = "blue", type = 'o')
lines(testing$usage ~ testing$day, col = "green", type = 'o')
par(new=TRUE)
plot(model3_forecast, col = color, axes = FALSE, shaded=TRUE, shadecols = color, fcol = "red")
par(new=F)

```

As we can see from the plots, linear model that was built on smaller ammount of data points is absolutelly different from the previous one. At the same time, ARIMA (2,0,0) model still gives acceptable results.  